{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run validation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Decrease Impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_mdi(fit, feature_names):\n",
    "    \"\"\"\n",
    "    \n",
    "    feature importance based on IS mean impurity reduction\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df0 = {i:tree.feature_importances_ for i, tree in enumerate(fit.estimateors_)}\n",
    "    df0 = pd.DataFrame.from_dict(df0, orient = 'index')\n",
    "    df0.columns = feature_names\n",
    "    df0 = df0.replace(0, np.nan) # because max_features = 1\n",
    "    imp = pd.concat({'mean':df0.mean(), 'std':df0.std()*df0.shape[0]**-0.5}, axis=1)\n",
    "    imp /= imp['mean'].sum()\n",
    "    \n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Decrease Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_mda(clf, X, y, cv, sample_weight, t1, pct_embargo, scoring='neg_log_loss'):\n",
    "    \"\"\"\n",
    "    \n",
    "    feature importance based on OOS score reduction\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if scoring not in ['neg_log_loss', 'accuracy']:\n",
    "        raise Exception ('wrong scoring method')\n",
    "    \n",
    "    cv_gen = PurgedKFold(n_splits=cv, t1=t1, pct_embargo=pct_embargo) # purged cv\n",
    "    scr_0, scr_1 = pd.Series(), pd.DataFrame(columns=X.columns)\n",
    "    for i, (train, test) in enumerate(cv_gen.split(X=X))\n",
    "        X0, y0, w0 = X.iloc[train, :], y.iloc[train], sample_eights.iloc[train]\n",
    "        X1, y1, w1 = X.iloc[test, :], y.iloc[test], sample_weights.iloc[test]\n",
    "        fit = clf.fit(X=X0, y= y0, sample_weight=w0.values)\n",
    "        if scoring=='neg_log_loss':\n",
    "            prob = fit.predict_probs(X1)\n",
    "            scr0.loc[i]=-log_loss(y1, prob, sample_weight=w1.values, labels=clr.classes_)\n",
    "        else:\n",
    "            pred = fit.predict(X1)\n",
    "            scr0.loc[i]=accuracy_score(y1, pred, sample_weight=w1.values)\n",
    "        for j in X.columns:\n",
    "            X1_ = X1.copy(deep=True)\n",
    "            np.random.suffle(X1_[j].values) # permuation of a single column\n",
    "            if scoring=='neg_log_loss':\n",
    "                prob = fit.predict_probs(X1_)\n",
    "                scr1.loc[i,j]=-log_loss(y1, prob, sample_weight=w1.values, labels=clr.classes_)\n",
    "            else:\n",
    "                pred = fit.predict(X1_)\n",
    "                scr0.loc[i,j]=accuracy_score(y1, pred, sample_weight=w1.values)\n",
    "    imp = (-src1).add(scr0, axis=0)\n",
    "    if scoring == 'neg_log_loss': imp = imp/-src1\n",
    "    else: imp = imp/(1.0-src1)\n",
    "    imp = pd.concat({'mean':imp.mean(), 'std':imp.std()*imp.shape[0]**-0.5}, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
